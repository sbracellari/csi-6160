{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "from functools import cache\n",
    "import polars as pl\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"column_1\"\n",
    "df = pl.read_csv(\"data/leaf.csv\", has_header=False)\n",
    "df = df.drop(\"column_2\")\n",
    "X = df.drop(\"column_1\").to_numpy()\n",
    "y = df.get_column(\"column_1\").to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=70)\n",
    "reg = LogisticRegression()\n",
    "# reg = RandomForestClassifier()\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import polars as pl\n",
    "import itertools\n",
    "\n",
    "TARGET = \"column_1\"\n",
    "\n",
    "def fetch_data(df, use_split, rbf):\n",
    "    assert(use_split >= 0 and use_split <= 4)\n",
    "    # Perform 5-fold cross validation with a deterministic seed\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "    splits = list(kf.split(df))\n",
    "    # Indexing the dataframe with an array returns the appropriate splits\n",
    "    df_train, df_test = df[splits[use_split][0]], df[splits[use_split][1]]\n",
    "    for col in df_train.columns:\n",
    "        if col == TARGET or col in rbf:\n",
    "            continue\n",
    "        df_train, df_test = scale_column(df_train, df_test, col)\n",
    "    cols = list(df_train.columns)\n",
    "    # binary = binary_columns(df_train)\n",
    "    for col in rbf:\n",
    "        if col == TARGET:\n",
    "            continue\n",
    "        df_train, df_test = rbf_column(df_train, df_test, col) # Use radial bias function\n",
    "    for col in cols:\n",
    "        if col == TARGET:\n",
    "            continue\n",
    "        if col not in rbf:\n",
    "            df_train, df_test = fourier_column(df_train, df_test, col)\n",
    "    if False:\n",
    "        for (col_a, col_b) in itertools.combinations(binary, 2):\n",
    "            if col_a == TARGET or col_b == TARGET:\n",
    "                continue\n",
    "            df_train, df_test = mul_cols(df_train, df_test, col_a, col_b)\n",
    "    return df_train, df_test\n",
    "\n",
    "def binary_columns(df):\n",
    "    binary = []\n",
    "    for col in df.columns:\n",
    "        if df.get_column(col).n_unique() == 2:\n",
    "            binary.append(col)\n",
    "    return binary\n",
    "\n",
    "def scale_column(df_train, df_test, col_name):\n",
    "    '''Scale a column from 0 to 1'''\n",
    "    max = df_train.get_column(col_name).max()\n",
    "    min = df_train.get_column(col_name).min()\n",
    "    df_train = df_train.with_columns((pl.col(col_name) - min) / (max - min))\n",
    "    df_test = df_test.with_columns((pl.col(col_name) - min) / (max - min))\n",
    "    return df_train, df_test\n",
    "\n",
    "def rbf_column(df_train, df_test, col_name):\n",
    "    '''Divide a col into 3 features using a radial basis function'''\n",
    "    std = df_train.get_column(col_name).std()\n",
    "    low = df_train.get_column(col_name).quantile(0.25)\n",
    "    middle = df_train.get_column(col_name).quantile(0.5)\n",
    "    high = df_train.get_column(col_name).quantile(0.75)\n",
    "    for i, val in enumerate([low, middle, high]):\n",
    "        df_train = df_train.with_columns((-1.0 * (pl.col(col_name) - val)**2 / (2 * std)**2).exp().alias(f\"{col_name}_{i}\"))\n",
    "        df_test = df_test.with_columns((-1.0 * (pl.col(col_name) - val)**2 / (2 * std)**2).exp().alias(f\"{col_name}_{i}\"))\n",
    "    return df_train.drop(col_name), df_test.drop(col_name)\n",
    "\n",
    "def fourier_column(df_train, df_test, col_name):\n",
    "    '''Divide a col in 3 features using math'''\n",
    "    for i in range(1, 4):\n",
    "        df_train = df_train.with_columns((pl.col(col_name) * pl.lit(i) * pl.lit(np.pi)).cos().alias(f\"{col_name}_{i - 1}\"))\n",
    "        df_test = df_test.with_columns((pl.col(col_name) * pl.lit(i) * pl.lit(np.pi)).cos().alias(f\"{col_name}_{i - 1}\"))\n",
    "    return df_train.drop(col_name), df_test.drop(col_name)\n",
    "\n",
    "def mul_cols(df_train, df_test, col_a, col_b):\n",
    "    df_train = df_train.with_columns((pl.col(col_a) * pl.col(col_b)).alias(f\"{col_a}+{col_b}\"))\n",
    "    df_test = df_test.with_columns((pl.col(col_a) * pl.col(col_b)).alias(f\"{col_a}+{col_b}\"))\n",
    "    return df_train, df_test\n",
    "\n",
    "def get_x_y(df):\n",
    "    X, y = df.drop(TARGET), df.get_column(TARGET)\n",
    "    X = X.with_columns(pl.lit(1.0).alias('constant')) # extra column for the bias term\n",
    "    return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "df = pl.read_csv(\"data/leaf.csv\", has_header=False)\n",
    "df = df.drop(\"column_2\")\n",
    "print(len(df.columns) - 1)\n",
    "df_train, df_test = fetch_data(df, 0, list(df.columns)[11:])\n",
    "X_train, y_train = get_x_y(df_train)\n",
    "X_test, y_test = get_x_y(df_test)\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rbf):\n",
    "    a = np.array(list(df.columns[1:]))\n",
    "    rbf = a[rbf > 0.5]\n",
    "    df_train, df_test = fetch_data(df, 0, rbf)\n",
    "    X_train, y_train = get_x_y(df_train)\n",
    "    X_test, y_test = get_x_y(df_test)\n",
    "    reg = LogisticRegression(solver=\"newton-cg\")\n",
    "    reg.fit(X_train, y_train)\n",
    "    return reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.random(14)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(df.columns[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(list(df.columns[1:]))\n",
    "a[arr > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(df.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression(solver=\"newton-cg\")\n",
    "# reg = RandomForestClassifier()\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation: Rbf per column, \n",
    "# [X_1 * X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(\"data\", train=True, download=True)\n",
    "test = MNIST(\"data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = train.data.flatten(start_dim=1)\n",
    "foo.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# X, y = load_digits(return_X_y=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)\n",
    "\n",
    "# model = RandomForestClassifier(\n",
    "#     n_estimators=100, max_depth=38, random_state=2023, min_samples_split=0.01, min_samples_leaf=25,\n",
    "#     max_features=1\n",
    "# )\n",
    "# Max Features\n",
    "crit = \"gini\"\n",
    "n_samples = 100\n",
    "model = RandomForestClassifier(criterion=crit, n_estimators=10, random_state=420, max_depth=5, min_samples_leaf=1, \n",
    "                               max_features=25, min_samples_split=2,ccp_alpha=0.01, min_impurity_decrease=0.01)\n",
    "model.fit(train.data[0:n_samples, :, :].flatten(start_dim=1), train.targets[0:n_samples])\n",
    "\n",
    "model.score(test.data[:, :, :].flatten(start_dim=1), test.targets[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIM = 14\n",
    "LIMIT = 4\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Simple check to avoid numerical errors with extreme x values\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "class Configuration:\n",
    "    def __init__(self, array):\n",
    "        self.data = array\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.data.tobytes())\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (self.data == other.data).all()\n",
    "\n",
    "    def build_classifier(self):\n",
    "        data = sigmoid(self.data)\n",
    "        if data[0] < 0.5:\n",
    "            crit = \"gini\"\n",
    "        else:\n",
    "            crit = \"entropy\"\n",
    "        \n",
    "        n_est = int(data[1] * 100) + 1\n",
    "        max_depth = int(data[2] * 10) + 1\n",
    "        min_samp_leaf = int(data[3] * 3) + 1\n",
    "        max_features = int(data[4] * 25) + 1\n",
    "        return RandomForestClassifier(n_estimators=n_est, criterion=crit, min_samples_leaf=min_samp_leaf,\n",
    "                                      max_features=max_features,max_depth=max_depth, random_state=8675309)\n",
    "        \n",
    "        \n",
    "\n",
    "class Solutions:\n",
    "    def __init__(self, num):\n",
    "        self.solutions = np.stack([Solutions.new_solution() for _ in range(num)])\n",
    "        self.employed = np.array([True] * (num // 2) + [False] * (num // 2))\n",
    "        self.failures = np.zeros_like(self.employed, np.int32)\n",
    "        self.onlooker = np.logical_not(self.employed) # Unemployed\n",
    "        self.best_fitness = -1\n",
    "        self.best_sol = None \n",
    "        self.fitness_cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def new_solution():\n",
    "        return rand.random(size=NUM_DIM)\n",
    "\n",
    "    def random_sol(self, exclude=-1):\n",
    "        if exclude == -1:\n",
    "            return rand.choice(self.solutions)\n",
    "        else:\n",
    "            rand_idx = rand.randint(0, self.solutions.shape[0])\n",
    "            if rand_idx == exclude:\n",
    "                return self.random_sol(exclude=exclude)\n",
    "            else:\n",
    "                return self.solutions[rand_idx]\n",
    "\n",
    "    def get_employed(self):\n",
    "        return self.solutions[self.employed]\n",
    "\n",
    "    def get_unemployed(self):\n",
    "        return self.solutions[np.logical_not(self.employed)]\n",
    "        \n",
    "    def get_onlooker(self):\n",
    "        return self.solutions[self.onlooker]\n",
    "\n",
    "    def get_scout(self):\n",
    "        return self.solutions[self.scout]\n",
    "\n",
    "    def fitness(self, x):\n",
    "        return fitness(Configuration(x))\n",
    "\n",
    "    def most_fit(self):\n",
    "        fit = np.array([self.fitness(x) for x in self.solutions])\n",
    "        idx = fit.argmax()\n",
    "        return fit[idx], self.solutions[idx]\n",
    "\n",
    "    def update_best(self):\n",
    "        best_fit, best_sol = self.most_fit()\n",
    "        if best_fit > self.best_fitness:\n",
    "            self.best_sol = best_sol\n",
    "            self.best_fitness = best_fit\n",
    "\n",
    "@cache\n",
    "def fitness(x: Configuration):\n",
    "    a = np.array(list(df.columns[1:]))\n",
    "    rbf = a[x.data > 0.5]\n",
    "    df_train, df_test = fetch_data(df, 0, rbf)\n",
    "    X_train, y_train = get_x_y(df_train)\n",
    "    X_test, y_test = get_x_y(df_test)\n",
    "    reg = LogisticRegression(solver=\"newton-cg\")\n",
    "    reg.fit(X_train, y_train)\n",
    "    return reg.score(X_test, y_test)\n",
    "    # model = x.build_classifier()\n",
    "    # model.fit(train.data[0:n_samples, :, :].flatten(start_dim=1), train.targets[0:n_samples])\n",
    "    # fit = model.score(test.data[:, :, :].flatten(start_dim=1), test.targets[:])\n",
    "    # return fit\n",
    "\n",
    "\n",
    "def basic_employed(sol: Solutions, initial_idx: int):\n",
    "    initial = sol.solutions[initial_idx]\n",
    "    a = 0.1 # Todo figure this out\n",
    "    idx = rand.randint(0, initial.size)\n",
    "    phi = rand.uniform(low=-a, high=a)\n",
    "    out = np.copy(initial)\n",
    "    sol_k = sol.random_sol(exclude=initial_idx)\n",
    "    out[idx] += phi * (out[idx] - sol_k[idx])\n",
    "    # Todo make sure values stay within expected range\n",
    "    return out # Greedy select this\n",
    "\n",
    "def basic_onlooker(sol: Solutions):\n",
    "    employed = sol.get_employed()\n",
    "    fitnesses = np.array([sol.fitness(x) for x in employed]) # Todo figure out how to cache fitness\n",
    "    total_fitness = np.sum(fitnesses)\n",
    "    bee_idx = rand.choice(np.arange(len(employed)), p=fitnesses/total_fitness)\n",
    "    return basic_employed(sol, bee_idx)\n",
    "\n",
    "def vanilla_abc(num_bees, epoches):\n",
    "    # init_bees()\n",
    "    sol = Solutions(num_bees)\n",
    "    for _ in range(epoches):\n",
    "        # Employed\n",
    "        for idx in sol.employed.nonzero()[0]:\n",
    "            # print(f\"Employed {idx}\")\n",
    "            candidate = basic_employed(sol, idx)\n",
    "            if sol.fitness(candidate) > sol.fitness(sol.solutions[idx]):\n",
    "                sol.solutions[idx] = candidate\n",
    "                sol.failures[idx] = 0\n",
    "            else:\n",
    "                sol.failures[idx] += 1\n",
    "        # Onlooker\n",
    "        for idx in sol.onlooker.nonzero()[0]:\n",
    "            # print(f\"Onlooker {idx}\")\n",
    "            candidate = basic_onlooker(sol)\n",
    "            if sol.fitness(candidate) > sol.fitness(sol.solutions[idx]):\n",
    "                sol.solutions[idx] = candidate\n",
    "        # Scout\n",
    "        for idx in sol.employed.nonzero()[0]:\n",
    "            # print(f\"Scout {idx}\")\n",
    "            if sol.failures[idx] >= LIMIT:\n",
    "                sol.failures[idx] = 0\n",
    "                sol.solutions[idx, :] = Solutions.new_solution()\n",
    "        # Mark best\n",
    "        sol.update_best()\n",
    "        print(sol.best_fitness)\n",
    "        print(sol.best_sol)\n",
    "\n",
    "\n",
    "vanilla_abc(24, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b3ac4f165dd888171771316b43e76578a4a4463e6a4b92e07e5d333979bdd5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
